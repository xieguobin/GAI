{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boston House Prices dataset\n",
      "===========================\n",
      "\n",
      "Notes\n",
      "------\n",
      "Data Set Characteristics:  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive\n",
      "    \n",
      "    :Median Value (attribute 14) is usually the target\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "http://archive.ics.uci.edu/ml/datasets/Housing\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      "**References**\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 线性回归、SGD、SVM、KNN、DTR、集成算法RFR、ExtraRFR、GDBTR\n",
    "# 从sklearn.datasets导入波士顿房价数据读取器。\n",
    "from sklearn.datasets import load_boston\n",
    "# 从读取房价数据存储在变量boston中。\n",
    "boston = load_boston()\n",
    "# 输出数据描述。\n",
    "print (boston.DESCR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max target value is 50.0\n",
      "The min target value is 5.0\n",
      "The average target value is 22.5328063241\n"
     ]
    }
   ],
   "source": [
    "# 从sklearn.cross_validation导入数据分割器。\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 导入numpy并重命名为np。\n",
    "import numpy as np\n",
    "\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "\n",
    "# 随机采样25%的数据构建测试样本，其余作为训练样本。\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=33, test_size=0.25)\n",
    "\n",
    "# 分析回归目标值的差异。\n",
    "print (\"The max target value is\", np.max(boston.target))\n",
    "print (\"The min target value is\", np.min(boston.target))\n",
    "print (\"The average target value is\", np.mean(boston.target))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\A\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\A\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# 从sklearn.preprocessing导入数据标准化模块。\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 分别初始化对特征和目标值的标准化器。\n",
    "ss_X = StandardScaler()\n",
    "ss_y = StandardScaler()\n",
    "\n",
    "# 分别对训练和测试数据的特征以及目标值进行标准化处理。\n",
    "X_train = ss_X.fit_transform(X_train)\n",
    "X_test = ss_X.transform(X_test)\n",
    "\n",
    "y_train = ss_y.fit_transform(y_train)\n",
    "y_test = ss_y.transform(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 从sklearn.linear_model导入LinearRegression。\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# 使用默认配置初始化线性回归器LinearRegression。\n",
    "lr = LinearRegression()\n",
    "# 使用训练数据进行参数估计。\n",
    "lr.fit(X_train, y_train)\n",
    "# 对测试数据进行回归预测。\n",
    "lr_y_predict = lr.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('LinearReg', LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)), ('SGD', SGDRegressor(alpha=0.0001, average=False, epsilon=0.1, eta0=0.01,\n",
      "       fit_intercept=True, l1_ratio=0.15, learning_rate='invscaling',\n",
      "       loss='squared_loss', n_iter=5, penalty='l2', power_t=0.25,\n",
      "       random_state=None, shuffle=True, verbose=0, warm_start=False))]\n"
     ]
    }
   ],
   "source": [
    "# 从sklearn.linear_model导入SGDRegressor。\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "# 使用默认配置初始化线性回归器SGDRegressor。\n",
    "sgdr = SGDRegressor()\n",
    "# 使用训练数据进行参数估计。\n",
    "sgdr.fit(X_train, y_train)\n",
    "# 对测试数据进行回归预测。\n",
    "sgdr_y_predict = sgdr.predict(X_test)\n",
    "\n",
    "estimators = []\n",
    "estimators.append(('LinearReg',  lr))\n",
    "estimators.append(('SGD'      ,  sgdr))\n",
    "print(estimators)   # 各个模型的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('LinearReg', LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)), ('SGD', SGDRegressor(alpha=0.0001, average=False, epsilon=0.1, eta0=0.01,\n",
      "       fit_intercept=True, l1_ratio=0.15, learning_rate='invscaling',\n",
      "       loss='squared_loss', n_iter=5, penalty='l2', power_t=0.25,\n",
      "       random_state=None, shuffle=True, verbose=0, warm_start=False))]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'copy_X': True, 'fit_intercept': False, 'n_jobs': 1, 'normalize': False}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从sklearn.linear_model导入LinearRegression。\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# 在初始化参数基础上 调优线性回归器LinearRegression。\n",
    "lr = LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)\n",
    "# 使用训练数据进行参数估计。\n",
    "lr.fit(X_train, y_train)\n",
    "# 对测试数据进行回归预测。\n",
    "lr_y_predict = lr.predict(X_test)\n",
    "\n",
    "# 从sklearn.linear_model导入SGDRegressor。\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "# 在初始化参数基础上 调优线性回归器SGDRegressor。\n",
    "sgdr = SGDRegressor(alpha=0.0001, average=False, epsilon=0.1, eta0=0.01,\n",
    "       fit_intercept=True, l1_ratio=0.15, learning_rate='invscaling',\n",
    "       loss='squared_loss', n_iter=5, penalty='l2', power_t=0.25,\n",
    "       random_state=None, shuffle=True, verbose=0, warm_start=False)\n",
    "# 使用训练数据进行参数估计。\n",
    "sgdr.fit(X_train, y_train)\n",
    "# 对测试数据进行回归预测。\n",
    "sgdr_y_predict = sgdr.predict(X_test)\n",
    "\n",
    "estimators = []\n",
    "estimators.append(('LinearReg',  lr))\n",
    "estimators.append(('SGD'      ,  sgdr))\n",
    "print(estimators)   # 各个模型的参数\n",
    "\n",
    "lr.set_params(copy_X=True, fit_intercept=False, n_jobs=1, normalize=False)\n",
    "lr.get_params(deep = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of default measurement of LinearRegression is 67.63%\n",
      "The value of R-squared of LinearRegression is 67.63%\n",
      "The mean squared error of LinearRegression is 25.10\n",
      "The mean absoluate error of LinearRegression is 3.53\n",
      "67.63%\n",
      "25.10\n",
      "3.53\n"
     ]
    }
   ],
   "source": [
    "# 使用LinearRegression模型自带的评估模块，并输出评估结果。\n",
    "print ('The value of default measurement of LinearRegression is',  \"%.2f%%\"%(100*lr.score(X_test, y_test)))\n",
    "\n",
    "# 从sklearn.metrics依次导入r2_score、mean_squared_error以及mean_absoluate_error用于回归性能的评估。\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# 使用r2_score模块，并输出评估结果。\n",
    "print ('The value of R-squared of LinearRegression is',  \"%.2f%%\"%(100*r2_score(y_test, lr_y_predict)))\n",
    "\n",
    "# 使用mean_squared_error模块，并输出评估结果。\n",
    "print ('The mean squared error of LinearRegression is',  \"%.2f\"%(mean_squared_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(lr_y_predict))))  \n",
    "\n",
    "# 使用mean_absolute_error模块，并输出评估结果。\n",
    "print ('The mean absoluate error of LinearRegression is', \"%.2f\"%(mean_absolute_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(lr_y_predict)))) \n",
    "\n",
    "LinearReg_R2  = \"%.2f%%\"%(100*lr.score(X_test, y_test))\n",
    "LinearReg_MSE = \"%.2f\"%(mean_squared_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(lr_y_predict)))\n",
    "LinearReg_MAE = \"%.2f\"%(mean_absolute_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(lr_y_predict))) \n",
    "print(LinearReg_R2 )\n",
    "print(LinearReg_MSE)\n",
    "print(LinearReg_MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of default measurement of SGDRegressor is 65.78%\n",
      "MSE of SGDRegressor 26.53\n",
      "MAE of SGDRegressor 3.51\n",
      "65.78%\n",
      "26.53\n",
      "3.51\n"
     ]
    }
   ],
   "source": [
    "# 使用SGDRegressor模型自带的评估模块，并输出评估结果。\n",
    "print ('The value of default measurement of SGDRegressor is', \"%.2f%%\"%(100*sgdr.score(X_test, y_test)))\n",
    "\n",
    "# 使用r2_score模块，并输出评估结果。\n",
    "# print ('R^2 of SGDRegressor', \"%.2f%%\"%(100*r2_score(y_test, sgdr_y_predict)))\n",
    "\n",
    "# 使用mean_squared_error模块，并输出评估结果。\n",
    "print ('MSE of SGDRegressor', \"%.2f\"%(mean_squared_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(sgdr_y_predict))))\n",
    "\n",
    "# 使用mean_absolute_error模块，并输出评估结果。\n",
    "print ('MAE of SGDRegressor', \"%.2f\"%(mean_absolute_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(sgdr_y_predict))))    \n",
    "\n",
    "SGD_R2  = \"%.2f%%\"%(100*sgdr.score(X_test, y_test))\n",
    "SGD_MSE = \"%.2f\"%(mean_squared_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(sgdr_y_predict)))\n",
    "SGD_MAE = \"%.2f\"%(mean_absolute_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(sgdr_y_predict))) \n",
    "print(SGD_R2 )\n",
    "print(SGD_MSE)\n",
    "print(SGD_MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('linear_svr', SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
      "  kernel='linear', max_iter=-1, shrinking=True, tol=0.001, verbose=False)), ('Poly_svr', SVR(C=1, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
      "  kernel='poly', max_iter=-1, shrinking=True, tol=0.001, verbose=False)), ('RBF_svr', SVR(C=1, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
      "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False))]\n"
     ]
    }
   ],
   "source": [
    "# 从sklearn.svm中导入支持向量机（回归）模型。\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# 使用线性核函数配置的支持向量机进行回归训练，并且对测试样本进行预测。\n",
    "linear_svr = SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',kernel='linear', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
    "linear_svr.fit(X_train, y_train)\n",
    "linear_svr_y_predict = linear_svr.predict(X_test)\n",
    "\n",
    "# 使用多项式核函数配置的支持向量机进行回归训练，并且对测试样本进行预测。\n",
    "poly_svr = SVR(C=1, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',kernel='poly',   max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
    "poly_svr.fit(X_train, y_train)\n",
    "poly_svr_y_predict = poly_svr.predict(X_test)\n",
    "\n",
    "# 使用径向基核函数配置的支持向量机进行回归训练，并且对测试样本进行预测。\n",
    "rbf_svr = SVR(C=1, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',kernel='rbf',    max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
    "rbf_svr.fit(X_train, y_train)\n",
    "rbf_svr_y_predict = rbf_svr.predict(X_test)\n",
    "\n",
    "estimators = []\n",
    "estimators.append(('linear_svr',  linear_svr))\n",
    "estimators.append(('Poly_svr'  ,  poly_svr))\n",
    "estimators.append(('RBF_svr'   ,  rbf_svr))\n",
    "print(estimators)   # 各个模型的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 of linear SVR 0.65171709743\n",
      "MSE of linear SVR 27.0063071393\n",
      "MAE of linear SVR 3.42667291687\n",
      "('65.17%', '27.01', '3.43')\n",
      "65.17%\n",
      "27.01\n",
      "3.43\n"
     ]
    }
   ],
   "source": [
    "# 使用R-squared、MSE和MAE指标对三种配置的支持向量机（回归）模型在相同测试集上进行性能评估。\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "print ('R^2 of linear SVR', linear_svr.score(X_test, y_test))\n",
    "print ('MSE of linear SVR', mean_squared_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(linear_svr_y_predict)))\n",
    "print ('MAE of linear SVR', mean_absolute_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(linear_svr_y_predict)))   \n",
    "B11 = (\"%.2f%%\"%(100*linear_svr.score(X_test, y_test)),\"%.2f\"%(mean_squared_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(linear_svr_y_predict)))  \n",
    ", \"%.2f\"%(mean_absolute_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(linear_svr_y_predict))))\n",
    "print (B11)\n",
    "\n",
    "linear_svr_R2  = \"%.2f%%\"%(100*linear_svr.score(X_test, y_test))\n",
    "linear_svr_MSE = \"%.2f\"%(mean_squared_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(linear_svr_y_predict)))\n",
    "linear_svr_MAE = \"%.2f\"%(mean_absolute_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(linear_svr_y_predict))) \n",
    "print(linear_svr_R2 )\n",
    "print(linear_svr_MSE)\n",
    "print(linear_svr_MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 of Poly SVR 0.404454058003\n",
      "MSE of Poly SVR 46.179403314\n",
      "MAEof Poly SVR 3.75205926674\n",
      "40.45%\n",
      "46.18\n",
      "3.75\n"
     ]
    }
   ],
   "source": [
    "print ('R^2 of Poly SVR', poly_svr.score(X_test, y_test))\n",
    "print ('MSE of Poly SVR', mean_squared_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(poly_svr_y_predict)))\n",
    "print ('MAEof Poly SVR', mean_absolute_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(poly_svr_y_predict)))\n",
    "\n",
    "Poly_svr_R2  = \"%.2f%%\"%(100*poly_svr.score(X_test, y_test))\n",
    "Poly_svr_MSE = \"%.2f\"%(mean_squared_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(poly_svr_y_predict)))\n",
    "Poly_svr_MAE = \"%.2f\"%(mean_absolute_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(poly_svr_y_predict))) \n",
    "print(Poly_svr_R2 )\n",
    "print(Poly_svr_MSE)\n",
    "print(Poly_svr_MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 of RBF SVR 0.756406891227\n",
      "MSE of RBF SVR 18.8885250008\n",
      "MAE of RBF SVR 2.60756329798\n",
      "75.64%\n",
      "18.89\n",
      "2.61\n"
     ]
    }
   ],
   "source": [
    "print ('R^2 of RBF SVR', rbf_svr.score(X_test, y_test))\n",
    "print ('MSE of RBF SVR', mean_squared_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(rbf_svr_y_predict)))\n",
    "print ('MAE of RBF SVR', mean_absolute_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(rbf_svr_y_predict)))\n",
    "\n",
    "RBF_svr_R2  = \"%.2f%%\"%(100*rbf_svr.score(X_test, y_test))\n",
    "RBF_svr_MSE = \"%.2f\"%(mean_squared_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(rbf_svr_y_predict)))\n",
    "RBF_svr_MAE = \"%.2f\"%(mean_absolute_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(rbf_svr_y_predict))) \n",
    "print(RBF_svr_R2 )\n",
    "print(RBF_svr_MSE)\n",
    "print(RBF_svr_MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 从sklearn.neighbors导入KNeighborRegressor（K近邻回归器）。\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# 初始化K近邻回归器，并且调整配置，使得预测的方式为平均回归：weights='uniform'。\n",
    "uni_knr = KNeighborsRegressor(weights='uniform')\n",
    "uni_knr.fit(X_train, y_train)\n",
    "uni_knr_y_predict = uni_knr.predict(X_test)\n",
    "\n",
    "# 初始化K近邻回归器，并且调整配置，使得预测的方式为根据距离加权回归：weights='distance'。\n",
    "dis_knr = KNeighborsRegressor(weights='distance')\n",
    "dis_knr.fit(X_train, y_train)\n",
    "dis_knr_y_predict = dis_knr.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 of uniform-weighted KNeighorRegression: 0.690345456461\n",
      "MSE of uniform-weighted KNeighorRegression: 24.0110141732\n",
      "MAE of uniform-weighted KNeighorRegression 2.96803149606\n",
      "69.03%\n",
      "24.01\n",
      "2.97\n"
     ]
    }
   ],
   "source": [
    "# 使用R-squared、MSE以及MAE三种指标对平均回归配置的K近邻模型在测试集上进行性能评估。\n",
    "print ('R^2 of uniform-weighted KNeighorRegression:', uni_knr.score(X_test, y_test))\n",
    "print ('MSE of uniform-weighted KNeighorRegression:', mean_squared_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(uni_knr_y_predict)))\n",
    "print ('MAE of uniform-weighted KNeighorRegression', mean_absolute_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(uni_knr_y_predict)))\n",
    "\n",
    "KNR_unW_R2  = \"%.2f%%\"%(100*uni_knr.score(X_test, y_test))\n",
    "KNR_unW_MSE = \"%.2f\"%(mean_squared_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(uni_knr_y_predict)))\n",
    "KNR_unW_MAE = \"%.2f\"%(mean_absolute_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(uni_knr_y_predict))) \n",
    "print(KNR_unW_R2 )\n",
    "print(KNR_unW_MSE)\n",
    "print(KNR_unW_MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 of distance-weighted KNeighorRegression: 0.719758997016\n",
      "MSE of distance-weighted KNeighorRegression: 21.7302501609\n",
      "MAE of distance-weighted KNeighorRegression: 2.80505687851\n",
      "71.98%\n",
      "21.73\n",
      "2.81\n"
     ]
    }
   ],
   "source": [
    "# 使用R-squared、MSE以及MAE三种指标对根据距离加权回归配置的K近邻模型在测试集上进行性能评估。\n",
    "print ('R^2 of distance-weighted KNeighorRegression:', dis_knr.score(X_test, y_test))\n",
    "print ('MSE of distance-weighted KNeighorRegression:', mean_squared_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(dis_knr_y_predict)))\n",
    "print ('MAE of distance-weighted KNeighorRegression:', mean_absolute_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(dis_knr_y_predict))\t)\n",
    "\n",
    "KNR_diW_R2  = \"%.2f%%\"%(100*dis_knr.score(X_test, y_test))\n",
    "KNR_diW_MSE = \"%.2f\"%(mean_squared_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(dis_knr_y_predict)))\n",
    "KNR_diW_MAE = \"%.2f\"%(mean_absolute_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(dis_knr_y_predict))) \n",
    "print(KNR_diW_R2 )\n",
    "print(KNR_diW_MSE)\n",
    "print(KNR_diW_MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 从sklearn.tree中导入DecisionTreeRegressor。\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "# 使用默认配置初始化DecisionTreeRegressor。\n",
    "dtr = DecisionTreeRegressor()\n",
    "# 用波士顿房价的训练数据构建回归树。\n",
    "dtr.fit(X_train, y_train)\n",
    "# 使用默认配置的单一回归树对测试数据进行预测，并将预测值存储在变量dtr_y_predict中。\n",
    "dtr_y_predict = dtr.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 of DecisionTreeRegressor: 0.531796337487\n",
      "MSE of DecisionTreeRegressor: 36.3051181102\n",
      "MAE of DecisionTreeRegressor: 3.45433070866\n",
      "53.18%\n",
      "36.31\n",
      "3.45\n"
     ]
    }
   ],
   "source": [
    "# 使用R-squared、MSE以及MAE指标对默认配置的回归树在测试集上进行性能评估。\n",
    "print ('R^2 of DecisionTreeRegressor:', dtr.score(X_test, y_test))\n",
    "print ('MSE of DecisionTreeRegressor:', mean_squared_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(dtr_y_predict)))\n",
    "print ('MAE of DecisionTreeRegressor:', mean_absolute_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(dtr_y_predict)))\n",
    "\n",
    "DTR_R2  = \"%.2f%%\"%(100*dtr.score(X_test, y_test))\n",
    "DTR_MSE = \"%.2f\"%(mean_squared_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(dtr_y_predict)))\n",
    "DTR_MAE = \"%.2f\"%(mean_absolute_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(dtr_y_predict))) \n",
    "print(DTR_R2 )\n",
    "print(DTR_MSE)\n",
    "print(DTR_MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 从sklearn.ensemble中导入RandomForestRegressor、ExtraTreesGressor以及GradientBoostingRegressor。\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor,GradientBoostingRegressor\n",
    "\n",
    "# 使用BaggingRegressor训练模型，并对测试数据做出预测，结果存储在变量bagr_y_predict中。\n",
    "bagr = BaggingRegressor()\n",
    "bagr.fit(X_train, y_train)\n",
    "bagr_y_predict = bagr.predict(X_test)\n",
    "\n",
    "# 使用RandomForestRegressor训练模型，并对测试数据做出预测，结果存储在变量rfr_y_predict中。\n",
    "rfr = RandomForestRegressor()\n",
    "rfr.fit(X_train, y_train)\n",
    "rfr_y_predict = rfr.predict(X_test)\n",
    "\n",
    "# 使用ExtraTreesRegressor训练模型，并对测试数据做出预测，结果存储在变量etr_y_predict中。\n",
    "etr = ExtraTreesRegressor()\n",
    "etr.fit(X_train, y_train)\n",
    "etr_y_predict = etr.predict(X_test)\n",
    "\n",
    "# 使用AdaBoostRegressor训练模型，并对测试数据做出预测，结果存储在变量gbr_y_predict中。\n",
    "AdaBoostr = AdaBoostRegressor()\n",
    "AdaBoostr.fit(X_train, y_train)\n",
    "AdaBoostr_y_predict = AdaBoostr.predict(X_test)\n",
    "\n",
    "# 使用GradientBoostingRegressor训练模型，并对测试数据做出预测，结果存储在变量gbr_y_predict中。\n",
    "gbr = GradientBoostingRegressor()\n",
    "gbr.fit(X_train, y_train)\n",
    "gbr_y_predict = gbr.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 of BaggingRegressor: 0.850978768438\n",
      "MSE of BaggingRegressor: 11.5552992126\n",
      "MAE of BaggingRegressor: 2.28708661417\n",
      "85.10%\n",
      "11.56\n",
      "2.29\n"
     ]
    }
   ],
   "source": [
    "# 使用R-squared、MSE以及MAE指标对默认配置的Bagging装袋回归在测试集上进行性能评估。\n",
    "print ('R^2 of BaggingRegressor:', bagr.score(X_test, y_test))\n",
    "print ('MSE of BaggingRegressor:', mean_squared_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(bagr_y_predict)))\n",
    "print ('MAE of BaggingRegressor:', mean_absolute_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(bagr_y_predict)))\n",
    "\n",
    "BagR_R2  = \"%.2f%%\"%(100*bagr.score(X_test, y_test))\n",
    "BagR_MSE = \"%.2f\"%(mean_squared_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(bagr_y_predict)))\n",
    "BagR_MAE = \"%.2f\"%(mean_absolute_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(bagr_y_predict))) \n",
    "print(BagR_R2 )\n",
    "print(BagR_MSE)\n",
    "print(BagR_MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 of RandomForestRegressor: 0.800446800984\n",
      "MSE of RandomForestRegressor: 15.4736133858\n",
      "MAE of RandomForestRegressor: 2.4605511811\n",
      "80.04%\n",
      "15.47\n",
      "2.46\n"
     ]
    }
   ],
   "source": [
    "# 使用R-squared、MSE以及MAE指标对默认配置的随机回归森林在测试集上进行性能评估。\n",
    "print ('R^2 of RandomForestRegressor:', rfr.score(X_test, y_test))\n",
    "print ('MSE of RandomForestRegressor:', mean_squared_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(rfr_y_predict)))\n",
    "print ('MAE of RandomForestRegressor:', mean_absolute_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(rfr_y_predict)))\n",
    "\n",
    "RFR_R2  = \"%.2f%%\"%(100*rfr.score(X_test, y_test))\n",
    "RFR_MSE = \"%.2f\"%(mean_squared_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(rfr_y_predict)))\n",
    "RFR_MAE = \"%.2f\"%(mean_absolute_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(rfr_y_predict))) \n",
    "print(RFR_R2 )\n",
    "print(RFR_MSE)\n",
    "print(RFR_MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 of ExtraTreesRegessor: 0.792313344563\n",
      "MSE of  ExtraTreesRegessor: 16.104292126\n",
      "MAE of ExtraTreesRegessor: 2.48622047244\n",
      "79.23%\n",
      "16.10\n",
      "2.49\n"
     ]
    }
   ],
   "source": [
    "# 使用R-squared、MSE以及MAE指标对默认配置的极端回归森林在测试集上进行性能评估。\n",
    "print ('R^2 of ExtraTreesRegessor:', etr.score(X_test, y_test))\n",
    "print ('MSE of  ExtraTreesRegessor:', mean_squared_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(etr_y_predict)))\n",
    "print ('MAE of ExtraTreesRegessor:', mean_absolute_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(etr_y_predict)))\n",
    "\n",
    "# 利用训练好的极端回归森林模型，输出每种特征对预测目标的贡献度。\n",
    "# print (np.sort(zip(etr.feature_importances_, boston.feature_names), axis=0))\n",
    "\n",
    "ETR_R2  = \"%.2f%%\"%(100*etr.score(X_test, y_test))\n",
    "ETR_MSE = \"%.2f\"%(mean_squared_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(etr_y_predict)))\n",
    "ETR_MAE = \"%.2f\"%(mean_absolute_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(etr_y_predict))) \n",
    "print(ETR_R2 )\n",
    "print(ETR_MSE)\n",
    "print(ETR_MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 of AdaBoostRegressor: 0.781390995082\n",
      "MSE of AdaBoostRegressor: 16.9512252444\n",
      "MAE of AdaBoostRegressor: 2.91340023548\n",
      "78.14%\n",
      "16.95\n",
      "2.91\n"
     ]
    }
   ],
   "source": [
    "# 使用R-squared、MSE以及MAE指标对默认配置的AdaBoost提升回归在测试集上进行性能评估。\n",
    "print ('R^2 of AdaBoostRegressor:', AdaBoostr.score(X_test, y_test))\n",
    "print ('MSE of AdaBoostRegressor:', mean_squared_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(AdaBoostr_y_predict)))\n",
    "print ('MAE of AdaBoostRegressor:', mean_absolute_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(AdaBoostr_y_predict)))\n",
    "\n",
    "AdaBoostr_R2  = \"%.2f%%\"%(100*AdaBoostr.score(X_test, y_test))\n",
    "AdaBoostr_MSE = \"%.2f\"%(mean_squared_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(AdaBoostr_y_predict)))\n",
    "AdaBoostr_MAE = \"%.2f\"%(mean_absolute_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(AdaBoostr_y_predict))) \n",
    "print(AdaBoostr_R2 )\n",
    "print(AdaBoostr_MSE)\n",
    "print(AdaBoostr_MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 of GradientBoostingRegressor: 0.841947632652\n",
      "MSE of GradientBoostingRegressor: 12.2555851729\n",
      "MAE of GradientBoostingRegressor: 2.28001916821\n",
      "84.19%\n",
      "12.26\n",
      "2.28\n"
     ]
    }
   ],
   "source": [
    "# 使用R-squared、MSE以及MAE指标对默认配置的梯度提升回归树在测试集上进行性能评估。\n",
    "print ('R^2 of GradientBoostingRegressor:', gbr.score(X_test, y_test))\n",
    "print ('MSE of GradientBoostingRegressor:', mean_squared_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(gbr_y_predict)))\n",
    "print ('MAE of GradientBoostingRegressor:', mean_absolute_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(gbr_y_predict)))\n",
    "\n",
    "GBR_R2  = \"%.2f%%\"%(100*gbr.score(X_test, y_test))\n",
    "GBR_MSE = \"%.2f\"%(mean_squared_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(gbr_y_predict)))\n",
    "GBR_MAE = \"%.2f\"%(mean_absolute_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(gbr_y_predict))) \n",
    "print(GBR_R2 )\n",
    "print(GBR_MSE)\n",
    "print(GBR_MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型和参数   R^2    MSE   MAE  \n",
      "LinearReg    67.63% 25.10 3.53\n",
      "SGD          65.67% 26.62 3.50\n",
      "linear_svr   65.17% 27.01 3.43\n",
      "Poly_svr     40.45% 46.18 3.75\n",
      "RBF_svr      75.64% 18.89 2.61\n",
      "KNR_unW      69.03% 24.01 2.97\n",
      "KNR_diW      71.98% 21.73 2.81\n",
      "DTR          53.18% 36.31 3.45\n",
      "BagR         85.10% 11.56 2.29\n",
      "RFR          80.04% 15.47 2.46\n",
      "ETR          79.23% 16.10 2.49\n",
      "AdaBoostr    78.14% 16.95 2.91\n",
      "GBR          84.19% 12.26 2.28\n"
     ]
    }
   ],
   "source": [
    "# 综合打印输出比较、评估结果。\n",
    "# print ('Index','线性回归','随机梯度','SVM(线性核)','SVM(多项核)','SVM(高斯核)') \n",
    "# print ('R^2  ', \"%.2f%%  \"%(100*lr.score(X_test, y_test)), \"%.2f%%  \"%(100*sgdr.score(X_test, y_test)),\n",
    "#        \"%.2f%%     \"%(100*linear_svr.score(X_test, y_test)),\"%.2f%%     \"%(100*poly_svr.score(X_test, y_test)),\"%.2f%%     \"%(100*rbf_svr.score(X_test, y_test)))\n",
    "print ('模型和参数  ','R^2   ' , 'MSE  ','MAE  ')\n",
    "print ('LinearReg   ',LinearReg_R2 ,LinearReg_MSE ,LinearReg_MAE )   \n",
    "print ('SGD         ',SGD_R2       ,SGD_MSE       ,SGD_MAE       ) \n",
    "print ('linear_svr  ',linear_svr_R2,linear_svr_MSE,linear_svr_MAE)\n",
    "print ('Poly_svr    ',Poly_svr_R2  ,Poly_svr_MSE  ,Poly_svr_MAE  )\n",
    "print ('RBF_svr     ',RBF_svr_R2   ,RBF_svr_MSE   ,RBF_svr_MAE   )\n",
    "print ('KNR_unW     ',KNR_unW_R2   ,KNR_unW_MSE   ,KNR_unW_MAE   )\n",
    "print ('KNR_diW     ',KNR_diW_R2   ,KNR_diW_MSE   ,KNR_diW_MAE   )\n",
    "print ('DTR         ',DTR_R2       ,DTR_MSE       ,DTR_MAE       )\n",
    "print ('BagR        ',BagR_R2      ,BagR_MSE      ,BagR_MAE      )\n",
    "print ('RFR         ',RFR_R2       ,RFR_MSE       ,RFR_MAE       )\n",
    "print ('ETR         ',ETR_R2       ,ETR_MSE       ,ETR_MAE       )\n",
    "print ('AdaBoostr   ',AdaBoostr_R2 ,AdaBoostr_MSE ,AdaBoostr_MAE )\n",
    "print ('GBR         ',GBR_R2       ,GBR_MSE       ,GBR_MAE       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class AdaBoostRegressor in module sklearn.ensemble.weight_boosting:\n",
      "\n",
      "class AdaBoostRegressor(BaseWeightBoosting, sklearn.base.RegressorMixin)\n",
      " |  An AdaBoost regressor.\n",
      " |  \n",
      " |  An AdaBoost [1] regressor is a meta-estimator that begins by fitting a\n",
      " |  regressor on the original dataset and then fits additional copies of the\n",
      " |  regressor on the same dataset but where the weights of instances are\n",
      " |  adjusted according to the error of the current prediction. As such,\n",
      " |  subsequent regressors focus more on difficult cases.\n",
      " |  \n",
      " |  This class implements the algorithm known as AdaBoost.R2 [2].\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <adaboost>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  base_estimator : object, optional (default=DecisionTreeRegressor)\n",
      " |      The base estimator from which the boosted ensemble is built.\n",
      " |      Support for sample weighting is required.\n",
      " |  \n",
      " |  n_estimators : integer, optional (default=50)\n",
      " |      The maximum number of estimators at which boosting is terminated.\n",
      " |      In case of perfect fit, the learning procedure is stopped early.\n",
      " |  \n",
      " |  learning_rate : float, optional (default=1.)\n",
      " |      Learning rate shrinks the contribution of each regressor by\n",
      " |      ``learning_rate``. There is a trade-off between ``learning_rate`` and\n",
      " |      ``n_estimators``.\n",
      " |  \n",
      " |  loss : {'linear', 'square', 'exponential'}, optional (default='linear')\n",
      " |      The loss function to use when updating the weights after each\n",
      " |      boosting iteration.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, optional (default=None)\n",
      " |      If int, random_state is the seed used by the random number generator;\n",
      " |      If RandomState instance, random_state is the random number generator;\n",
      " |      If None, the random number generator is the RandomState instance used\n",
      " |      by `np.random`.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  estimators_ : list of classifiers\n",
      " |      The collection of fitted sub-estimators.\n",
      " |  \n",
      " |  estimator_weights_ : array of floats\n",
      " |      Weights for each estimator in the boosted ensemble.\n",
      " |  \n",
      " |  estimator_errors_ : array of floats\n",
      " |      Regression error for each estimator in the boosted ensemble.\n",
      " |  \n",
      " |  feature_importances_ : array of shape = [n_features]\n",
      " |      The feature importances if supported by the ``base_estimator``.\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  AdaBoostClassifier, GradientBoostingRegressor, DecisionTreeRegressor\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  .. [1] Y. Freund, R. Schapire, \"A Decision-Theoretic Generalization of\n",
      " |         on-Line Learning and an Application to Boosting\", 1995.\n",
      " |  \n",
      " |  .. [2] H. Drucker, \"Improving Regressors using Boosting Techniques\", 1997.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      AdaBoostRegressor\n",
      " |      BaseWeightBoosting\n",
      " |      abc.NewBase\n",
      " |      sklearn.ensemble.base.BaseEnsemble\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, base_estimator=None, n_estimators=50, learning_rate=1.0, loss='linear', random_state=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Build a boosted regressor from the training set (X, y).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
      " |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      " |          DOK, or LIL. DOK and LIL are converted to CSR.\n",
      " |      \n",
      " |      y : array-like of shape = [n_samples]\n",
      " |          The target values (real numbers).\n",
      " |      \n",
      " |      sample_weight : array-like of shape = [n_samples], optional\n",
      " |          Sample weights. If None, the sample weights are initialized to\n",
      " |          1 / n_samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Returns self.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict regression value for X.\n",
      " |      \n",
      " |      The predicted regression value of an input sample is computed\n",
      " |      as the weighted median prediction of the classifiers in the ensemble.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
      " |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      " |          DOK, or LIL. DOK and LIL are converted to CSR.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array of shape = [n_samples]\n",
      " |          The predicted regression values.\n",
      " |  \n",
      " |  staged_predict(self, X)\n",
      " |      Return staged predictions for X.\n",
      " |      \n",
      " |      The predicted regression value of an input sample is computed\n",
      " |      as the weighted median prediction of the classifiers in the ensemble.\n",
      " |      \n",
      " |      This generator method yields the ensemble prediction after each\n",
      " |      iteration of boosting and therefore allows monitoring, such as to\n",
      " |      determine the prediction on a test set after each boost.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
      " |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      " |          DOK, or LIL. DOK and LIL are converted to CSR.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : generator of array, shape = [n_samples]\n",
      " |          The predicted regression values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseWeightBoosting:\n",
      " |  \n",
      " |  staged_score(self, X, y, sample_weight=None)\n",
      " |      Return staged scores for X, y.\n",
      " |      \n",
      " |      This generator method yields the ensemble score after each iteration of\n",
      " |      boosting and therefore allows monitoring, such as to determine the\n",
      " |      score on a test set after each boost.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
      " |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      " |          DOK, or LIL. DOK and LIL are converted to CSR.\n",
      " |      \n",
      " |      y : array-like, shape = [n_samples]\n",
      " |          Labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      z : float\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from BaseWeightBoosting:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      Return the feature importances (the higher, the more important the\n",
      " |         feature).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : array, shape = [n_features]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.ensemble.base.BaseEnsemble:\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      Returns the index'th estimator in the ensemble.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Returns iterator over estimators in the ensemble.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Returns the number of estimators in the ensemble.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the coefficient of determination R^2 of the prediction.\n",
      " |      \n",
      " |      The coefficient R^2 is defined as (1 - u/v), where u is the regression\n",
      " |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\n",
      " |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      " |      Best possible score is 1.0 and it can be negative (because the\n",
      " |      model can be arbitrarily worse). A constant model that always\n",
      " |      predicts the expected value of y, disregarding the input features,\n",
      " |      would get a R^2 score of 0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True values for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          R^2 of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "help(sklearn.ensemble.AdaBoostRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LinearRegression in module sklearn.linear_model.base:\n",
      "\n",
      "class LinearRegression(LinearModel, sklearn.base.RegressorMixin)\n",
      " |  Ordinary least squares Linear Regression.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  fit_intercept : boolean, optional\n",
      " |      whether to calculate the intercept for this model. If set\n",
      " |      to false, no intercept will be used in calculations\n",
      " |      (e.g. data is expected to be already centered).\n",
      " |  \n",
      " |  normalize : boolean, optional, default False\n",
      " |      If True, the regressors X will be normalized before regression.\n",
      " |      This parameter is ignored when `fit_intercept` is set to False.\n",
      " |      When the regressors are normalized, note that this makes the\n",
      " |      hyperparameters learnt more robust and almost independent of the number\n",
      " |      of samples. The same property is not valid for standardized data.\n",
      " |      However, if you wish to standardize, please use\n",
      " |      `preprocessing.StandardScaler` before calling `fit` on an estimator\n",
      " |      with `normalize=False`.\n",
      " |  \n",
      " |  copy_X : boolean, optional, default True\n",
      " |      If True, X will be copied; else, it may be overwritten.\n",
      " |  \n",
      " |  n_jobs : int, optional, default 1\n",
      " |      The number of jobs to use for the computation.\n",
      " |      If -1 all CPUs are used. This will only provide speedup for\n",
      " |      n_targets > 1 and sufficient large problems.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  coef_ : array, shape (n_features, ) or (n_targets, n_features)\n",
      " |      Estimated coefficients for the linear regression problem.\n",
      " |      If multiple targets are passed during the fit (y 2D), this\n",
      " |      is a 2D array of shape (n_targets, n_features), while if only\n",
      " |      one target is passed, this is a 1D array of length n_features.\n",
      " |  \n",
      " |  residues_ : array, shape (n_targets,) or (1,) or empty\n",
      " |      Sum of residuals. Squared Euclidean 2-norm for each target passed\n",
      " |      during the fit. If the linear regression problem is under-determined\n",
      " |      (the number of linearly independent rows of the training matrix is less\n",
      " |      than its number of linearly independent columns), this is an empty\n",
      " |      array. If the target vector passed during the fit is 1-dimensional,\n",
      " |      this is a (1,) shape array.\n",
      " |  \n",
      " |      .. versionadded:: 0.18\n",
      " |  \n",
      " |  intercept_ : array\n",
      " |      Independent term in the linear model.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  From the implementation point of view, this is just plain Ordinary\n",
      " |  Least Squares (scipy.linalg.lstsq) wrapped as a predictor object.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LinearRegression\n",
      " |      LinearModel\n",
      " |      abc.NewBase\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, fit_intercept=True, normalize=False, copy_X=True, n_jobs=1)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit linear model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : numpy array or sparse matrix of shape [n_samples,n_features]\n",
      " |          Training data\n",
      " |      \n",
      " |      y : numpy array of shape [n_samples, n_targets]\n",
      " |          Target values\n",
      " |      \n",
      " |      sample_weight : numpy array of shape [n_samples]\n",
      " |          Individual weights for each sample\n",
      " |      \n",
      " |          .. versionadded:: 0.17\n",
      " |             parameter *sample_weight* support to LinearRegression.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : returns an instance of self.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  residues_\n",
      " |      DEPRECATED: ``residues_`` is deprecated and will be removed in 0.19\n",
      " |      \n",
      " |      Get the residues of the fitted model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from LinearModel:\n",
      " |  \n",
      " |  decision_function(*args, **kwargs)\n",
      " |      DEPRECATED:  and will be removed in 0.19.\n",
      " |      \n",
      " |      Decision function of the linear model.\n",
      " |      \n",
      " |              Parameters\n",
      " |              ----------\n",
      " |              X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      " |                  Samples.\n",
      " |      \n",
      " |              Returns\n",
      " |              -------\n",
      " |              C : array, shape = (n_samples,)\n",
      " |                  Returns predicted values.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict using the linear model\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array, shape = (n_samples,)\n",
      " |          Returns predicted values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the coefficient of determination R^2 of the prediction.\n",
      " |      \n",
      " |      The coefficient R^2 is defined as (1 - u/v), where u is the regression\n",
      " |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\n",
      " |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      " |      Best possible score is 1.0 and it can be negative (because the\n",
      " |      model can be arbitrarily worse). A constant model that always\n",
      " |      predicts the expected value of y, disregarding the input features,\n",
      " |      would get a R^2 score of 0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True values for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          R^2 of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import sklearn\n",
    "help(sklearn.linear_model.LinearRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
